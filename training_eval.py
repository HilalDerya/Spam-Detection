# -*- coding: utf-8 -*-
"""roberta_1_1_1_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1onQTYO7DLd7ROiI6SkjtmO7m_9Qb4OJv
"""

!pip install transformers[torch]
!pip install accelerate -U

import torch
import pandas as pd
from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available
from transformers import RobertaTokenizerFast, RobertaForSequenceClassification
from transformers import Trainer, TrainingArguments
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
import random
from imblearn.over_sampling import SMOTE
from sklearn.feature_extraction.text import TfidfVectorizer

def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    if is_torch_available():
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # ^^ safe to call this function even if cuda is not available
    if is_tf_available():
        import tensorflow as tf

        tf.random.set_seed(seed)

set_seed(1)

emails_df = pd.read_csv("/content/latest.csv")

model_name = "roberta-base"
max_length=300

tokenizer = RobertaTokenizerFast.from_pretrained(model_name)
model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)

emails_df = emails_df[emails_df['subject'].notna()]
emails_df = emails_df[emails_df["body"].notna()]

def prepare_data(df, include_xlabels= True):
    texts=[]
    labels=[]
    for i in range(len(df)):
        text = df["body"].iloc[i]
        label = df["is_spam"].iloc[i]
        if text and label in [0,1]:
            texts.append(text)
            labels.append(label)
    return train_test_split(texts,labels, test_size=0.2, random_state=42)

from sklearn.utils import resample

minority_class = 1
majority_class = 0

minority_df = emails_df[emails_df['is_spam'] == minority_class]
majority_df = emails_df[emails_df['is_spam'] == majority_class]

minority_upsampled = resample(minority_df, replace=True, n_samples=len(majority_df), random_state=42)

emails_df_balanced = pd.concat([majority_df, minority_upsampled])

# Process the balanced data
emails_df_balanced = emails_df_balanced[emails_df_balanced['subject'].notna()]
emails_df_balanced = emails_df_balanced[emails_df_balanced["body"].notna()]

# Split the balanced data into train and validation sets
train_texts, valid_texts, train_labels, valid_labels = prepare_data(emails_df_balanced)

print(len(train_texts), len(train_labels))
print(len(valid_texts), len(valid_labels))

class NewsGroupsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        #item["labels"] = torch.nn.functional.one_hot(torch.tensor([self.labels[idx]]), num_classes=2).squeeze(0)
        item["labels"] = torch.tensor([self.labels[idx]])
        return item

    def __len__(self):
        return len(self.labels)

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)
valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)
train_dataset = NewsGroupsDataset(train_encodings, train_labels)
valid_dataset = NewsGroupsDataset(valid_encodings, valid_labels)

from scipy.special import softmax
import matplotlib.pyplot as plt

def compute_metrics(p):
    eval_pred = p.predictions
    labels = p.label_ids
    pred = np.argmax(eval_pred, axis=1)
    acc = accuracy_score(y_true=labels, y_pred=pred)

    f1 = f1_score(labels, pred, average='binary')

    return {'f1': f1,
            'acc': acc}

training_args = TrainingArguments(
    output_dir='results',
    num_train_epochs=5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    save_strategy="epoch",
    #logging_dir='logs',
    logging_steps=500,
    save_steps=500,
    logging_strategy="epoch",
    evaluation_strategy="epoch",
    eval_steps=100,
    save_total_limit=2,
    metric_for_best_model='f1',
    greater_is_better=True,
    load_best_model_at_end=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

#trainer.state.log_history
train_loss = [elem['loss'] for elem in trainer.state.log_history if 'loss' in elem]
eval_loss = [elem['eval_loss'] for elem in trainer.state.log_history if 'eval_loss' in elem]

model_path = ('/content')
model.save_pretrained('/content/sample_data')

import seaborn as sns

y_true = valid_labels
y_pred_probs = trainer.predict(valid_dataset).predictions
y_pred = np.argmax(y_pred_probs, axis=1)

def plot_confusion_matrix(y_true, y_pred, labels=['Not Spam', 'Spam']):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

plot_confusion_matrix(y_true, y_pred)

from sklearn.metrics import roc_auc_score, roc_curve

y_pred_probs = trainer.predict(valid_dataset).predictions
y_pred = np.argmax(y_pred_probs, axis=1)

def plot_roc_curve(y_true, y_pred_probs):
    roc_auc = roc_auc_score(y_true, y_pred_probs[:,1])
    fpr, tpr, _ = roc_curve(y_true, y_pred_probs[:,1])

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}'.format(roc_auc))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc='lower right')
    plt.show()

plot_roc_curve(valid_labels, y_pred_probs)

min_length = min(len(train_loss), len(eval_loss))
epochs = range(1, min_length + 1)

plt.plot(epochs, train_loss[:min_length], label='Training Loss')
plt.plot(epochs, eval_loss[:min_length], label='Evaluation Loss')

plt.title('Training and Evaluation Loss Over Time')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

def create_results_table(y_true, y_probs):
    thresholds = np.arange(0, 1.5, 0.5)
    results_list = []

    for threshold in thresholds:
        predicted_labels = (y_probs[:, 1] >= threshold).astype(int)

        results_dict = {'Threshold': threshold,
                        'Precision': precision_score(y_true, predicted_labels),
                        'Recall': recall_score(y_true, predicted_labels),
                        'F1 Score': f1_score(y_true, predicted_labels),
                        'Accuracy': accuracy_score(y_true, predicted_labels)}

        tn, fp, fn, tp = confusion_matrix(y_true, predicted_labels).ravel()
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        results_dict['Specificity'] = specificity

        results_list.append(results_dict)

    results_df = pd.DataFrame(results_list)
    return results_df

# Assuming y_pred_probs is a 2D array where y_pred_probs[:, 1] contains the predicted probabilities for the positive class
results_table = create_results_table(valid_labels, y_pred_probs)
print(results_table)

trainer.evaluate()

def get_prediction(text, convert_to_label=False):
    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors="pt").to("cuda")
    outputs = model(**inputs)
    probs = outputs[0].softmax(1)
    d = {
        0: "Not Spam",
        1: "Spam"
    }
    if convert_to_label:
      return d[int(probs.argmax())]
    else:
      return int(probs.argmax())

email="""
"""

get_prediction(email, convert_to_label=True)